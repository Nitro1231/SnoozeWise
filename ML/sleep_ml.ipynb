{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2978856-826f-48ec-a3ab-6b0601311fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting coremltools\n",
      "  Downloading coremltools-7.1-cp311-none-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from coremltools) (1.26.0)\n",
      "Collecting protobuf<=4.0.0,>=3.1.0 (from coremltools)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: sympy in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from coremltools) (1.12)\n",
      "Requirement already satisfied: tqdm in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from coremltools) (4.66.1)\n",
      "Requirement already satisfied: packaging in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from coremltools) (23.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from coremltools) (23.1.0)\n",
      "Collecting cattrs (from coremltools)\n",
      "  Downloading cattrs-23.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyaml (from coremltools)\n",
      "  Downloading pyaml-23.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from pyaml->coremltools) (6.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nitro/miniconda3/envs/test/lib/python3.11/site-packages (from sympy->coremltools) (1.3.0)\n",
      "Downloading coremltools-7.1-cp311-none-macosx_11_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: pyaml, protobuf, cattrs, coremltools\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.4\n",
      "    Uninstalling protobuf-4.24.4:\n",
      "      Successfully uninstalled protobuf-4.24.4\n",
      "Successfully installed cattrs-23.2.3 coremltools-7.1 protobuf-3.20.3 pyaml-23.12.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e265b7b3-5f6d-4e38-9ba1-b39522be01a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "TensorFlow version 2.14.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import coremltools as ct\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743ac78c-eecd-4f42-b136-56ecb57e1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE = {\n",
    "    'In Bed': 0,\n",
    "    'Awake': 1,\n",
    "    'Asleep': 2,\n",
    "    'REM': 3,\n",
    "    'Core': 4,\n",
    "    'Deep': 5,\n",
    "    'Unknown': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27430de8-50ec-47d6-8592-e67a3ff2cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: str) -> dict:\n",
    "    with open(path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def parse_time(time: str) -> datetime:\n",
    "    return datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def to_min(cur_time: datetime, start_time: datetime) -> int:\n",
    "    return int((cur_time - start_time).total_seconds()//60)\n",
    "\n",
    "def process_sleep_data(sleep_data: dict) -> dict:\n",
    "    stage_data = defaultdict(dict)\n",
    "\n",
    "    for data in sleep_data:\n",
    "        start_time = parse_time(data['start_time'])\n",
    "        end_time = parse_time(data['end_time'])\n",
    "        date = end_time.strftime('%Y-%m-%d')\n",
    "\n",
    "        if not date in stage_data: stage_data[date]['stages'] = list()\n",
    "        stage_data[date]['stages'].append({\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'stage': STAGE[data['stage']]\n",
    "        })\n",
    "\n",
    "    for date, data in stage_data.items():\n",
    "        stage_data[date]['stages'] = sorted(data['stages'], key=lambda x: x['start_time'])\n",
    "        stage_data[date]['start_time'] = stage_data[date]['stages'][0]['start_time']\n",
    "        stage_data[date]['end_time'] = stage_data[date]['stages'][-1]['end_time']\n",
    "        stage_data[date]['day_of_week'] = stage_data[date]['start_time'].weekday()\n",
    "\n",
    "    return stage_data\n",
    "\n",
    "def post_process(stage_data: dict) -> pd.DataFrame:\n",
    "    ml_data = list()\n",
    "    for date, data in stage_data.items():\n",
    "        for item in data['stages']:\n",
    "            start_time = to_min(item['start_time'], data['start_time'])\n",
    "            end_time = to_min(item['end_time'], data['start_time'])\n",
    "            for time in range(start_time, end_time):\n",
    "                ml_data.append((\n",
    "                    date,\n",
    "                    time,\n",
    "                    item['stage']\n",
    "                ))\n",
    "    return pd.DataFrame(ml_data, columns=['date', 'day_of_week', 'time', 'stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "580e1b32-9785-4951-aef4-7b9ea32c1cb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "4 columns passed, passed data had 3 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.11/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.11/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 4 columns passed, passed data had 3 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msleep_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m stage_data \u001b[38;5;241m=\u001b[39m process_sleep_data(data)\n\u001b[0;32m----> 5\u001b[0m ml_data \u001b[38;5;241m=\u001b[39m \u001b[43mpost_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m ml_data\n",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m, in \u001b[0;36mpost_process\u001b[0;34m(stage_data)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_time, end_time):\n\u001b[1;32m     41\u001b[0m             ml_data\u001b[38;5;241m.\u001b[39mappend((\n\u001b[1;32m     42\u001b[0m                 date,\n\u001b[1;32m     43\u001b[0m                 time,\n\u001b[1;32m     44\u001b[0m                 item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m             ))\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday_of_week\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.11/site-packages/pandas/core/frame.py:809\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 809\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    818\u001b[0m         arrays,\n\u001b[1;32m    819\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.11/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.11/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 4 columns passed, passed data had 3 columns"
     ]
    }
   ],
   "source": [
    "data = load_json('./sleepData.json')\n",
    "data = data['sleep_data']\n",
    "\n",
    "stage_data = process_sleep_data(data)\n",
    "ml_data = post_process(stage_data)\n",
    "\n",
    "ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3774065f-6972-4612-b5a0-6e6a35ef763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current backend accelerator: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Current backend accelerator:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7200e14b-a6a1-48d5-a4c2-ecba65f1c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = ml_data[['day_of_week', 'time']].values\n",
    "y = ml_data['stage'].values\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class SleepStageDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = SleepStageDataset(X_train, y_train)\n",
    "val_dataset = SleepStageDataset(X_val, y_val)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed08ac5-4a39-41ac-a023-86203c258551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dim_model=128, num_heads=4, num_encoder_layers=3, dropout_rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_layer = nn.Linear(num_features, dim_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(dim_model, dropout_rate)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout_rate)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding_layer(src)\n",
    "        src = src.unsqueeze(1)  # Add batch dimension\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.squeeze(1)  # Remove batch dimension\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d13828-daca-4233-9e0f-be9120b7bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ecc3b5a-45c6-4a15-b2b6-2bf8c53b1e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitro/miniconda3/envs/cs178/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0038286174064155\n",
      "Epoch 2, Loss: 0.9987461775893995\n",
      "Epoch 3, Loss: 0.9988278262033768\n",
      "Epoch 4, Loss: 0.9987478831362543\n",
      "Epoch 5, Loss: 0.998903670153775\n",
      "Epoch 6, Loss: 0.9988938988590281\n",
      "Epoch 7, Loss: 0.998983971906837\n",
      "Epoch 8, Loss: 0.9988751759719566\n",
      "Epoch 9, Loss: 0.9988846839708689\n",
      "Epoch 10, Loss: 0.9989667202795549\n",
      "Accuracy: 70.21849982826495%\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "num_features = 2  # day_of_week and time\n",
    "num_classes = len(set(y_train))  # Assuming y_train is accessible here\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = TransformerModel(num_features, num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e6e832-5f8c-44e1-8d78-818e5149df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████▉| 257/258 [00:00<00:00, 7121.48 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|█████████| 5/5 [00:00<00:00, 1077.78 passes/s]\n",
      "Running MIL default pipeline:   0%|                           | 0/71 [00:00<?, ? passes/s]/Users/nitro/miniconda3/envs/cs178/lib/python3.10/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:239: UserWarning: Input, 'src.1', of the source model, has been renamed to 'src_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|█████████████████| 71/71 [00:00<00:00, 281.73 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████| 12/12 [00:00<00:00, 1873.01 passes/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to TorchScript\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "torch.save(model, 'pytorchmodel.pth')\n",
    "\n",
    "X_data, _ = list(train_loader)[0]\n",
    "\n",
    "traced_model = torch.jit.trace(model, X_data)\n",
    "traced_model.save('traced_model.pt')\n",
    "\n",
    "model = ct.convert(\n",
    "    traced_model,\n",
    "    convert_to='mlprogram',\n",
    "    inputs=[ct.TensorType(shape=X_data.shape)]\n",
    ")\n",
    " \n",
    "# Save the converted model.\n",
    "model.save('sleepCoreML.mlpackage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9c1f6-391a-4355-b520-f73f754f91ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
